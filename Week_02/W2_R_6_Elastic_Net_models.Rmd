---
title: "Ridge Regression"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(111)
require(knitr)
require(plotly)
require(ggplot2)
require(ISLR)
require(glmnet)
```

## Data 

```{r}
Hitters <- Hitters
str(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters) # here, complete cases analysis is performed 
# N59 A values are in the dependent variable column (Salary) only.
```
## Model/task description

We will use the data included in the Hitters dataset to predict players' **Salary** (our dependent variable).

## Dataset description

| Variable   | Description                                                                         |   |
|------------|-------------------------------------------------------------------------------------|---|
| AtBat      |  Number of times at bat in 1986                                                     |   |
| Hits       |  Number of hits in 1986                                                             |   |
| HmRun      |  Number of home runs in 1986                                                        |   |
| Runs       |  Number of runs in 1986                                                             |   |
| RBI        |  Number of runs batted in in 1986                                                   |   |
| Walks      |  Number of walks in 1986                                                            |   |
| Years      |  Number of years in the major   leagues                                             |   |
| CAtBat     |  Number of times at bat during his   career                                         |   |
| CHits      |  Number of hits during his career                                                   |   |
| CHmRun     |  Number of home runs during his   career                                            |   |
| CRuns      |  Number of runs during his career                                                   |   |
| CRBI       |  Number of runs batted in during   his career                                       |   |
| Cwalks     |  Number of walks during his career                                                  |   |
| League     |  A factor with levels A and N   indicating player’s league at the end of 1986       |   |
| Division   |  A factor with levels E and W   indicating player’s division at the end of 1986     |   |
| PutOuts    |  Number of put outs in 1986                                                         |   |
| Assists    |  Number of assists in 1986                                                          |   |
| Errors     |  Number of errors in 1986                                                           |   |
| Salary     |  1987 annual salary on opening day   in thousands of dollars                        |   |
| NewLeague  |  A factor with levels A and N   indicating player`s league at the beginning of 1987 |   |


---


# Ridge regression


## Data preparation (for ridge, lasso and elastic net penalties):


`model.matrix()` is from the `{stats}` package,


`glmnet()` function from the `{glmnet}` package needs regression input in matrix $\mathbf{X}$ and vector $\mathbf{y}$ form.

```{r}
x <-  model.matrix(Salary~., Hitters)[ , -1]
y <- Hitters$Salary
```


The function `glmnet()` performs the **elastic net** estimation.

* Regressor standardization is performed automatically (by default, may be turned off).


**Elastic net** is based on a general formula that encompasses both lasso and ridge regression


* alpha = 0 - ridge regression
* alpha = 1 - lasso regression
* 0 < alpha < 1 - elastic net regression


For illustration, we estimate model for lambda < 0.001 , 10^7 > 


* Otherwise, lambda (regularization/penality) coefficient can be selected automatically 

```{r}
grid <- 10^seq(-3, 7, length=100)
ridge.mod <- glmnet(x, y ,alpha=0, lambda=grid)
dim(coef(ridge.mod))
```

`ridge.mod` is a dataframe object that stores model coefficients (20 coefficients) for each of the 100 models estimated (for each lambda from the `grid` object, a model is estimated)

```{r}
plot(ridge.mod, xvar="lambda", label=TRUE)
```

Coefficient value plot as a function of log of lambda. `glmnet()` estimates multiple models on a grid of values of lambda.


When log of lambda is 12, all the coefficients are essentially zero. Then as we relax the lambda restriction parameter, the coefficients grow away from zero in a nice smooth way, and the sum of squares of the coefficients is getting bigger and bigger until we reach a point where lambda is effectively zero, and the coefficients are unregularized (equal to OLS from `lm()`, up to rounding errors).

## Sample coefficients for different lambda values:

**High lambda**

```{r}
#value of lambda
ridge.mod$lambda[3]
log(ridge.mod$lambda[3])
coef(ridge.mod)[,3]
```
**Lower lambda**
```{r}
#value of lambda
ridge.mod$lambda[80]
log(ridge.mod$lambda[80])
coef(ridge.mod)[,80]
```

## Model selection by cross-validation

Ridge regression gives a whole *path of models* and we need to pick one. 

glmnet's got a built-in function, called `CV.glmnet`, that will do cross validation


```{r}
cv.ridge <- cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

With high lambda (right hand side of the plot), the mean squared error (*MSE*) is very high and the coefficients are restricted to be small. 


For lower lambda values, at some point, the *MSE* kind of levels off. The plot seems to indicate that the full model is doing a good job.

* Note the two two vertical lines. One is at the minimum *MSE* and the other vertical line is within one standard error of the minimum. (for lambda with minimum MSE, use: `s=lambda.min`)

* The second line is a more restricted model that does almost as well as the minimum, and sometimes we'll go for that - the model for the largest value of lambda such that error is within 1 standard error of the minimum. (for this lambda, use: `s=lambda.1se`)

* At the top of the plot, you can see how many non-zero variables coefficients are in the model. There's all 20 variables in the model (19 variables plus the intercept) and no coefficient is zero (as we are doing ridge regression, regressors are not turned off).

## Coefficient extraction

There's a coefficient function extractor that works on a cross validation object and pick the coefficient vector corresponding to the best model:

```{r}
coef(cv.ridge, s="lambda.min") # alternatively, use "lambda.1se"
cv.ridge$lambda.min
```

## Predictions
Sample fitted values for the first 5 observations in the Hitters dataset
```{r}
predict(cv.ridge, newx=x[1:5,], s="lambda.min")
predict(cv.ridge, newx=x[1:5,], s="lambda.1se")
```


---


#  Lasso regression

## Model/task description

We perform the same task: will use the data included in the Hitters dataset to predict players' **Salary** (our dependent variable). 


However, lasso performs both


* shrinkage
* and variable selection.

## Plots

For illustration, we estimate the lasso model for lambda < 0.001 , 10^6 > 

```{r}
grid <- 10^seq(-3, 6, length=100)
lasso.mod <- glmnet(x, y ,alpha=1, lambda=grid)
dim(coef(lasso.mod))
```

Again, `lasso.mod` is a 100 x 20 dataframe.

```{r}
plot(lasso.mod, xvar="lambda", label=TRUE)
```

As lambda values increase, different coefficients are shrunken or turned off.



## Model selection by cross-validation

```{r}
cv.lasso <- cv.glmnet(x,y,alpha=1)
plot(cv.lasso)
```


## Coefficient extraction for `lambda.min` and `lambda.1se`:

```{r}
coef(cv.lasso, s="lambda.min")
coef(cv.lasso, s="lambda.1se") # s="lambda.1se" is the default setting
cv.lasso$lambda.min
cv.lasso$lambda.1se
```


## Predictions
Sample fitted values for the first 5 observations in the Hitters dataset
```{r}
predict(cv.lasso, newx=x[1:5,], s="lambda.min")
```

---


#  Elastic net regression

## Model/task description

For this illustration, we set $\alpha = 0.5$.

## Plots

For illustration, we estimate the elastic net model for lambda < 0.001 , 10^6 > 

```{r}
grid <- 10^seq(-3, 6, length=100)
EN.mod <- glmnet(x, y ,alpha=0.5, lambda=grid)
```

Again, `EN.mod` is a 100 x 20 dataframe.

```{r}
plot(EN.mod, xvar="lambda", label=TRUE)
```


## Model selection by cross-validation

```{r}
cv.EN <- cv.glmnet(x,y,alpha=0.5)
plot(cv.EN)
```


## Coefficient extraction for `lambda.min` and `lambda.1se`:

```{r}
coef(cv.EN, s="lambda.min")
coef(cv.EN, s="lambda.1se") # s="lambda.1se" is the default setting
cv.EN$lambda.min
cv.EN$lambda.1se
```


## Predictions
Sample fitted values for the first 5 observations in the Hitters dataset
```{r}
predict(cv.EN, newx=x[1:5,], s="lambda.min")
```